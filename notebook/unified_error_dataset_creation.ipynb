{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Unified Error Dataset Creation\n",
        "\n",
        "This notebook creates a single unified dataset containing all data points where models made mistakes during evaluation.\n",
        "\n",
        "## Dataset Structure:\n",
        "- **Email datasets**: Concatenate `subject` + `body` → `content`\n",
        "- **SMS datasets**: Rename `message` → `content`\n",
        "- **Final columns**: `id`, `content`, `label`\n",
        "- **Deduplication**: Remove repetitive data points based on content\n",
        "\n",
        "## Purpose:\n",
        "Create a clean, unified dataset of problematic cases for:\n",
        "- Error pattern analysis\n",
        "- Model improvement\n",
        "- Training data augmentation\n",
        "- Failure case studies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNIFIED ERROR DATASET CREATION ===\n",
            "Loading all model error cases from evaluation results...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Set\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"=== UNIFIED ERROR DATASET CREATION ===\")\n",
        "print(\"Loading all model error cases from evaluation results...\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load All Detailed Results and Filter for Errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning results for evaluation results...\n",
            "Processing dataset: annotated\n",
            "Processing dataset: phishing_sms_dataset\n",
            "  Found 6 error cases in 20250613_230316\n",
            "  Found 590 error cases in 20250616_040647\n",
            "  Found 4 error cases in 20250616_031241\n",
            "  Found 1 error cases in 20250613_230006\n",
            "  Found 6 error cases in 20250614_011431\n",
            "  Found 627 error cases in 20250614_030314\n",
            "  Found 4 error cases in 20250614_011218\n",
            "  Found 15 error cases in 20250616_030752\n",
            "  Found 8 error cases in 20250613_225308\n",
            "Processing dataset: unified_phishing_email_dataset\n",
            "  Found 9 error cases in 20250613_111708\n",
            "  Found 99 error cases in 20250613_032254\n",
            "  Found 5 error cases in 20250613_030731\n",
            "  Found 5 error cases in 20250613_024907\n",
            "  Found 5 error cases in 20250613_024938\n",
            "  Found 5 error cases in 20250613_025015\n",
            "  Found 1 error cases in 20250611_210318\n",
            "  Found 13 error cases in 20250613_035449\n",
            "  Found 5 error cases in 20250613_024820\n",
            "  Found 4 error cases in 20250613_035701\n",
            "  Found 46 error cases in 20250613_224943\n",
            "  Found 1 error cases in 20250613_030829\n",
            "  Found 1 error cases in 20250613_034441\n",
            "  Found 10 error cases in 20250613_222658\n",
            "  Found 7 error cases in 20250613_035325\n",
            "  Found 5 error cases in 20250613_024859\n",
            "  Found 10 error cases in 20250613_221444\n",
            "  Found 73 error cases in 20250613_131858\n",
            "  Found 1 error cases in 20250613_024832\n",
            "  Found 16 error cases in 20250613_035144\n",
            "  Found 1155 error cases in 20250613_061121\n",
            "  Found 14 error cases in 20250613_114859\n",
            "  Found 5 error cases in 20250613_024947\n",
            "  Found 763 error cases in 20250613_184137\n",
            "  Found 9 error cases in 20250613_110804\n",
            "  Found 2 error cases in 20250613_025045\n",
            "  Found 2 error cases in 20250613_025042\n",
            "  Found 9 error cases in 20250613_111152\n",
            "  Found 13 error cases in 20250613_121539\n",
            "  Found 2 error cases in 20250612_161644\n",
            "  Found 1 error cases in 20250613_030824\n",
            "  Found 15 error cases in 20250613_212421\n",
            "  Found 482 error cases in 20250613_034032\n",
            "  Found 111 error cases in 20250613_124929\n",
            "  Found 5 error cases in 20250613_030729\n",
            "  Found 1 error cases in 20250612_161657\n",
            "  Found 7 error cases in 20250613_120238\n",
            "  Found 2 error cases in 20250611_210301\n",
            "  Found 6 error cases in 20250613_034318\n",
            "\n",
            "✅ Total error cases loaded: 4186\n",
            "📊 Error cases by dataset:\n",
            "  - unified_phishing_email_dataset: 2925 errors\n",
            "  - phishing_sms_dataset: 1261 errors\n",
            "🤖 Error cases by model:\n",
            "  - unsloth/qwen3-30b-a3b: 1762 errors\n",
            "  - Deepseek-r1-0528-distill-qwen3-32b-preview0-qat: 1473 errors\n",
            "  - qwen3-0.6b: 482 errors\n",
            "  - unsloth/qwen3-32b: 124 errors\n",
            "  - Qwen/Qwen3-30B-A3B: 101 errors\n",
            "  - qwen3-235b-a22b-128k: 71 errors\n",
            "  - Qwen/Qwen2.5-1.5B-Instruct: 44 errors\n",
            "  - unsloth/qwen3-4b: 16 errors\n",
            "  - josiefied-deepseek-r1-0528-qwen3-8b-abliterated-v1: 15 errors\n",
            "  - gemma-3-27b-it: 15 errors\n",
            "  - llama-4-maverick-17b-128e-instruct: 14 errors\n",
            "  - qwen/deepseek-r1-0528-qwen3-8b: 13 errors\n",
            "  - gpt-4.1: 12 errors\n",
            "  - qwen/qwen3-235b-a22b: 10 errors\n",
            "  - llama-4-scout-17b-16e-instruct: 9 errors\n",
            "  - qwen/qwen3-8b: 7 errors\n",
            "  - deepseek-r1-0528-distill-qwen3-32b-preview0-qat: 7 errors\n",
            "  - gemma3-1b-it: 6 errors\n",
            "  - gemini-2.5-flash-preview-05-20: 4 errors\n",
            "  - unsloth/gemma-3-4b-it: 1 errors\n",
            "\n",
            "📋 Available columns: ['record_id', 'actual_label', 'actual_class', 'predicted_label', 'predicted_class', 'is_correct', 'llm_reason', 'original_message', 'dataset_name', 'model', 'provider', 'sample_size', 'timestamp', 'run_id', 'id', 'original_subject', 'original_body', 'original_source']\n",
            "📏 Dataset shape: (4186, 18)\n"
          ]
        }
      ],
      "source": [
        "def load_all_error_cases(results_base_dir: str = 'results') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load all detailed results and filter for incorrect predictions only.\n",
        "    Returns a DataFrame containing all error cases from all evaluations.\n",
        "    \"\"\"\n",
        "    error_cases_list = []\n",
        "    results_path = Path(results_base_dir)\n",
        "    \n",
        "    if not results_path.exists():\n",
        "        print(f\"Results directory {results_base_dir} not found!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"Scanning {results_base_dir} for evaluation results...\")\n",
        "    \n",
        "    # Iterate through dataset directories\n",
        "    for dataset_dir in results_path.iterdir():\n",
        "        if not dataset_dir.is_dir():\n",
        "            continue\n",
        "            \n",
        "        dataset_name = dataset_dir.name\n",
        "        print(f\"Processing dataset: {dataset_name}\")\n",
        "        \n",
        "        # Iterate through timestamp directories\n",
        "        for timestamp_dir in dataset_dir.iterdir():\n",
        "            if not timestamp_dir.is_dir():\n",
        "                continue\n",
        "                \n",
        "            # Load detailed results and evaluation info\n",
        "            detailed_file = timestamp_dir / 'detailed_results.csv'\n",
        "            info_file = timestamp_dir / 'evaluation_info.json'\n",
        "            \n",
        "            if detailed_file.exists() and info_file.exists():\n",
        "                try:\n",
        "                    # Load evaluation info for metadata\n",
        "                    with open(info_file, 'r') as f:\n",
        "                        info_data = json.load(f)\n",
        "                    \n",
        "                    # Load detailed results\n",
        "                    detailed_df = pd.read_csv(detailed_file)\n",
        "                    \n",
        "                    # Filter for incorrect predictions only\n",
        "                    error_cases = detailed_df[detailed_df['is_correct'] == False].copy()\n",
        "                    \n",
        "                    if len(error_cases) > 0:\n",
        "                        # Add metadata to each error case\n",
        "                        error_cases['dataset_name'] = dataset_name\n",
        "                        error_cases['model'] = info_data['evaluation_config']['model']\n",
        "                        error_cases['provider'] = info_data['evaluation_config']['provider']\n",
        "                        error_cases['sample_size'] = info_data['evaluation_config']['sample_size']\n",
        "                        error_cases['timestamp'] = info_data['timestamp']\n",
        "                        error_cases['run_id'] = f\"{dataset_name}_{info_data['timestamp']}\"\n",
        "                        \n",
        "                        error_cases_list.append(error_cases)\n",
        "                        print(f\"  Found {len(error_cases)} error cases in {timestamp_dir.name}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"  Error loading {timestamp_dir}: {e}\")\n",
        "                    continue\n",
        "    \n",
        "    if error_cases_list:\n",
        "        combined_errors = pd.concat(error_cases_list, ignore_index=True)\n",
        "        print(f\"\\n✅ Total error cases loaded: {len(combined_errors)}\")\n",
        "        print(f\"📊 Error cases by dataset:\")\n",
        "        for dataset, count in combined_errors['dataset_name'].value_counts().items():\n",
        "            print(f\"  - {dataset}: {count} errors\")\n",
        "        print(f\"🤖 Error cases by model:\")\n",
        "        for model, count in combined_errors['model'].value_counts().items():\n",
        "            print(f\"  - {model}: {count} errors\")\n",
        "        return combined_errors\n",
        "    else:\n",
        "        print(\"❌ No error cases found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load all error cases\n",
        "all_errors = load_all_error_cases()\n",
        "\n",
        "if len(all_errors) > 0:\n",
        "    print(f\"\\n📋 Available columns: {all_errors.columns.tolist()}\")\n",
        "    print(f\"📏 Dataset shape: {all_errors.shape}\")\n",
        "else:\n",
        "    print(\"No error data to process\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Process Content Based on Dataset Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found original content columns: ['original_message', 'original_subject', 'original_body', 'original_source']\n",
            "\n",
            "📧 Processing phishing_sms_dataset (1261 error cases)\n",
            "  Detected as SMS dataset (based on dataset name)\n",
            "\n",
            "📧 Processing unified_phishing_email_dataset (2925 error cases)\n",
            "  Detected as EMAIL dataset (based on dataset name)\n",
            "\n",
            "✅ Successfully processed 4186 error cases with content\n",
            "\n",
            "📊 Content processing summary:\n",
            "  Total records with content: 4186\n",
            "  Average content length: 2622 characters\n",
            "  Content length range: 3 - 112031\n",
            "\n",
            "📄 Sample processed content:\n",
            "\n",
            "Example 1 (phishing_sms_dataset):\n",
            "Label: SCAM\n",
            "Error Type: False Negative\n",
            "Source: sms\n",
            "Content: SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 2 (phishing_sms_dataset):\n",
            "Label: LEGITIMATE\n",
            "Error Type: False Positive\n",
            "Source: sms\n",
            "Content: R Ì_ going 4 today's meeting?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def process_content_by_dataset_type(error_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process content based on dataset type:\n",
        "    - Email datasets: concatenate subject + body → content\n",
        "    - SMS datasets: rename message → content\n",
        "    Returns DataFrame with standardized 'content' column\n",
        "    \"\"\"\n",
        "    if len(error_df) == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    processed_records = []\n",
        "    \n",
        "    # Get original content columns (those starting with 'original_')\n",
        "    original_columns = [col for col in error_df.columns if col.startswith('original_')]\n",
        "    print(f\"Found original content columns: {original_columns}\")\n",
        "    \n",
        "    for dataset_name in error_df['dataset_name'].unique():\n",
        "        dataset_errors = error_df[error_df['dataset_name'] == dataset_name].copy()\n",
        "        print(f\"\\n📧 Processing {dataset_name} ({len(dataset_errors)} error cases)\")\n",
        "        \n",
        "        # Determine dataset type based on dataset name (parent directory)\n",
        "        is_email_dataset = False\n",
        "        is_sms_dataset = False\n",
        "        \n",
        "        # Simple categorization based on dataset name\n",
        "        if 'email' in dataset_name.lower():\n",
        "            is_email_dataset = True\n",
        "            print(f\"  Detected as EMAIL dataset (based on dataset name)\")\n",
        "        elif 'sms' in dataset_name.lower():\n",
        "            is_sms_dataset = True\n",
        "            print(f\"  Detected as SMS dataset (based on dataset name)\")\n",
        "        else:\n",
        "            # Fallback: check columns to determine best processing approach\n",
        "            if 'original_subject' in dataset_errors.columns and 'original_body' in dataset_errors.columns:\n",
        "                is_email_dataset = True\n",
        "                print(f\"  Detected as EMAIL dataset (has subject & body columns)\")\n",
        "            elif 'original_message' in dataset_errors.columns or 'original_text' in dataset_errors.columns:\n",
        "                is_sms_dataset = True\n",
        "                print(f\"  Detected as SMS dataset (has message/text column)\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ Warning: Unable to determine dataset type. Available columns: {original_columns}\")\n",
        "                # Try to find any text-like columns\n",
        "                text_columns = [col for col in original_columns if any(text_word in col.lower() \n",
        "                               for text_word in ['text', 'content', 'message', 'body', 'subject'])]\n",
        "                if text_columns:\n",
        "                    print(f\"  Using available text columns: {text_columns}\")\n",
        "        \n",
        "        # Process each error case\n",
        "        for idx, row in dataset_errors.iterrows():\n",
        "            # Determine source type\n",
        "            if is_email_dataset:\n",
        "                source_type = 'email'\n",
        "            elif is_sms_dataset:\n",
        "                source_type = 'sms'\n",
        "            else:\n",
        "                source_type = 'other'\n",
        "            \n",
        "            record = {\n",
        "                'dataset_name': dataset_name,\n",
        "                'model': row['model'],\n",
        "                'run_id': row['run_id'],\n",
        "                'actual_label': row['actual_label'],\n",
        "                'predicted_label': row['predicted_label'],\n",
        "                'error_type': 'False Negative' if (row['actual_label'] == 1 and row['predicted_label'] == 0) else 'False Positive',\n",
        "                'llm_reason': row.get('llm_reason', ''),\n",
        "                'content': '',\n",
        "                'source': source_type,\n",
        "                'original_id': row.get('original_id', '') or row.get('id', '')\n",
        "            }\n",
        "            \n",
        "            # Create content based on dataset type\n",
        "            if is_email_dataset:\n",
        "                # Email: concatenate subject + body\n",
        "                subject = str(row.get('original_subject', '')).strip()\n",
        "                body = str(row.get('original_body', '')).strip()\n",
        "                \n",
        "                # Handle NaN values\n",
        "                if subject == 'nan' or pd.isna(row.get('original_subject')):\n",
        "                    subject = ''\n",
        "                if body == 'nan' or pd.isna(row.get('original_body')):\n",
        "                    body = ''\n",
        "                \n",
        "                # Concatenate with proper formatting\n",
        "                if subject and body:\n",
        "                    record['content'] = f\"Subject: {subject}\\n\\nBody: {body}\"\n",
        "                elif subject:\n",
        "                    record['content'] = f\"Subject: {subject}\"\n",
        "                elif body:\n",
        "                    record['content'] = f\"Body: {body}\"\n",
        "                else:\n",
        "                    record['content'] = ''\n",
        "            \n",
        "            elif is_sms_dataset:\n",
        "                # SMS: use message or text column\n",
        "                if 'original_message' in row:\n",
        "                    message = str(row.get('original_message', '')).strip()\n",
        "                elif 'original_text' in row:\n",
        "                    message = str(row.get('original_text', '')).strip()\n",
        "                else:\n",
        "                    message = ''\n",
        "                \n",
        "                # Handle NaN values\n",
        "                if message == 'nan' or pd.isna(message):\n",
        "                    message = ''\n",
        "                \n",
        "                record['content'] = message\n",
        "            \n",
        "            else:\n",
        "                # Fallback: concatenate all available text columns\n",
        "                content_parts = []\n",
        "                for col in original_columns:\n",
        "                    if col != 'original_id':\n",
        "                        value = str(row.get(col, '')).strip()\n",
        "                        if value and value != 'nan' and not pd.isna(value):\n",
        "                            col_name = col.replace('original_', '').title()\n",
        "                            content_parts.append(f\"{col_name}: {value}\")\n",
        "                \n",
        "                record['content'] = '\\n\\n'.join(content_parts)\n",
        "            \n",
        "            # Only add if content is not empty\n",
        "            if record['content'].strip():\n",
        "                processed_records.append(record)\n",
        "    \n",
        "    if processed_records:\n",
        "        processed_df = pd.DataFrame(processed_records)\n",
        "        print(f\"\\n✅ Successfully processed {len(processed_df)} error cases with content\")\n",
        "        return processed_df\n",
        "    else:\n",
        "        print(f\"\\n❌ No valid content found in error cases\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Process content for all error cases\n",
        "if len(all_errors) > 0:\n",
        "    processed_errors = process_content_by_dataset_type(all_errors)\n",
        "    \n",
        "    if len(processed_errors) > 0:\n",
        "        print(f\"\\n📊 Content processing summary:\")\n",
        "        print(f\"  Total records with content: {len(processed_errors)}\")\n",
        "        print(f\"  Average content length: {processed_errors['content'].str.len().mean():.0f} characters\")\n",
        "        print(f\"  Content length range: {processed_errors['content'].str.len().min()} - {processed_errors['content'].str.len().max()}\")\n",
        "        \n",
        "        # Show sample of processed content\n",
        "        print(f\"\\n📄 Sample processed content:\")\n",
        "        for i, (_, row) in enumerate(processed_errors.head(2).iterrows()):\n",
        "            print(f\"\\nExample {i+1} ({row['dataset_name']}):\")\n",
        "            print(f\"Label: {'SCAM' if row['actual_label'] == 1 else 'LEGITIMATE'}\")\n",
        "            print(f\"Error Type: {row['error_type']}\")\n",
        "            print(f\"Source: {row['source']}\")\n",
        "            content_preview = row['content'][:200] + \"...\" if len(row['content']) > 200 else row['content']\n",
        "            print(f\"Content: {content_preview}\")\n",
        "            print(\"-\" * 50)\n",
        "    else:\n",
        "        print(\"❌ No content could be processed\")\n",
        "else:\n",
        "    print(\"❌ No error data available to process\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Deduplicate Data Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Deduplicating 4186 error cases...\n",
            "✅ Deduplication complete!\n",
            "  Before: 4186 error cases\n",
            "  After: 2540 unique data points\n",
            "  Duplicates removed: 1646\n",
            "\\n📊 Duplicate statistics:\n",
            "  1451 data points appeared in 1 error case (unique)\n",
            "  915 data points appeared in 2 error cases (duplicated)\n",
            "  61 data points appeared in 3 error cases (duplicated)\n",
            "  37 data points appeared in 4 error cases (duplicated)\n",
            "  16 data points appeared in 5 error cases (duplicated)\n",
            "  34 data points appeared in 6 error cases (duplicated)\n",
            "  11 data points appeared in 7 error cases (duplicated)\n",
            "  2 data points appeared in 8 error cases (duplicated)\n",
            "  3 data points appeared in 9 error cases (duplicated)\n",
            "  2 data points appeared in 11 error cases (duplicated)\n",
            "  2 data points appeared in 13 error cases (duplicated)\n",
            "  1 data points appeared in 15 error cases (duplicated)\n",
            "  3 data points appeared in 16 error cases (duplicated)\n",
            "  1 data points appeared in 20 error cases (duplicated)\n",
            "  1 data points appeared in 39 error cases (duplicated)\n",
            "\\n🚨 Most problematic data points (failed by multiple models/runs):\n",
            "\\n1. Failed 39 times\n",
            "   Label: SCAM\n",
            "   Failed models: gpt-4.1, Qwen/Qwen3-30B-A3B, Qwen/Qwen2.5-1.5B-Instruct, qwen/deepseek-r1-0528-qwen3-8b, unsloth/qwen3-30b-a3b, qwen3-235b-a22b-128k, unsloth/gemma-3-4b-it, qwen/qwen3-8b, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-4b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemini-2.5-flash-preview-05-20, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat, gemma3-1b-it\n",
            "   Content preview: Subject: CNN.com Daily Top 10\n",
            "\n",
            "Body: >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+= >...\n",
            "\\n2. Failed 20 times\n",
            "   Label: SCAM\n",
            "   Failed models: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen/deepseek-r1-0528-qwen3-8b, unsloth/qwen3-30b-a3b, qwen3-235b-a22b-128k, qwen/qwen3-8b, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-4b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Newsletter Coach'Invest - Septembre 2002\n",
            "\n",
            "Body: NEWSLETTER COACHINVEST  SEPTEMBRE 2002\n",
            "\n",
            "Coa...\n",
            "\\n3. Failed 16 times\n",
            "   Label: SCAM\n",
            "   Failed models: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: failure notice\n",
            "\n",
            "Body: hi . this is the qmail - send program at bouncehost .\n",
            "i ' m afraid i ...\n",
            "\\n4. Failed 16 times\n",
            "   Label: SCAM\n",
            "   Failed models: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Introduction Regular Expressions are a widely-used method of specifying patterns of text to...\n",
            "\\n5. Failed 16 times\n",
            "   Label: SCAM\n",
            "   Failed models: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Re: Re:\n",
            "\n",
            "Body: The cop looked like a big doll that has been badly treated by a gang of nast...\n",
            "\\n📋 Deduplicated dataset shape: (2540, 10)\n",
            "📋 Columns: ['label', 'source_datasets', 'failed_models', 'source_runs', 'error_types', 'source', 'original_id', 'content', 'llm_reason', 'error_count']\n"
          ]
        }
      ],
      "source": [
        "def deduplicate_error_cases(processed_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Remove duplicate data points based on content.\n",
        "    Keep the first occurrence and track which models/runs had errors on each data point.\n",
        "    \"\"\"\n",
        "    if len(processed_df) == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"🔍 Deduplicating {len(processed_df)} error cases...\")\n",
        "    \n",
        "    # Find duplicates based on content and label (case-insensitive content comparison)\n",
        "    processed_df['content_normalized'] = processed_df['content'].str.lower().str.strip()\n",
        "    \n",
        "    # Group by normalized content and label to find duplicates\n",
        "    duplicates_info = processed_df.groupby(['content_normalized', 'actual_label']).agg({\n",
        "        'dataset_name': lambda x: ', '.join(x.unique()),\n",
        "        'model': lambda x: ', '.join(x.unique()),\n",
        "        'run_id': lambda x: ', '.join(x.unique()),\n",
        "        'error_type': lambda x: ', '.join(x.unique()),\n",
        "        'source': 'first',  # Keep the source type\n",
        "        'original_id': 'first',\n",
        "        'content': 'first',  # Keep original content (not normalized)\n",
        "        'llm_reason': lambda x: ' | '.join([r for r in x.unique() if r and str(r).strip()])\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Count how many times each content appears\n",
        "    duplicate_counts = processed_df.groupby(['content_normalized', 'actual_label']).size().reset_index(name='error_count')\n",
        "    \n",
        "    # Merge back the counts\n",
        "    deduplicated = duplicates_info.merge(duplicate_counts, on=['content_normalized', 'actual_label'])\n",
        "    \n",
        "    # Drop the normalized content column (we don't need it in final output)\n",
        "    deduplicated = deduplicated.drop('content_normalized', axis=1)\n",
        "    \n",
        "    # Rename columns for clarity\n",
        "    deduplicated = deduplicated.rename(columns={\n",
        "        'actual_label': 'label',\n",
        "        'dataset_name': 'source_datasets',\n",
        "        'model': 'failed_models',\n",
        "        'run_id': 'source_runs',\n",
        "        'error_type': 'error_types'\n",
        "    })\n",
        "    \n",
        "    print(f\"✅ Deduplication complete!\")\n",
        "    print(f\"  Before: {len(processed_df)} error cases\")\n",
        "    print(f\"  After: {len(deduplicated)} unique data points\")\n",
        "    print(f\"  Duplicates removed: {len(processed_df) - len(deduplicated)}\")\n",
        "    \n",
        "    # Show duplicate statistics\n",
        "    duplicate_stats = deduplicated['error_count'].value_counts().sort_index()\n",
        "    print(f\"\\\\n📊 Duplicate statistics:\")\n",
        "    for count, freq in duplicate_stats.items():\n",
        "        if count == 1:\n",
        "            print(f\"  {freq} data points appeared in 1 error case (unique)\")\n",
        "        else:\n",
        "            print(f\"  {freq} data points appeared in {count} error cases (duplicated)\")\n",
        "    \n",
        "    # Show most problematic data points (those that multiple models got wrong)\n",
        "    most_problematic = deduplicated.nlargest(5, 'error_count')\n",
        "    if len(most_problematic) > 0:\n",
        "        print(f\"\\\\n🚨 Most problematic data points (failed by multiple models/runs):\")\n",
        "        for i, (_, row) in enumerate(most_problematic.iterrows(), 1):\n",
        "            print(f\"\\\\n{i}. Failed {row['error_count']} times\")\n",
        "            print(f\"   Label: {'SCAM' if row['label'] == 1 else 'LEGITIMATE'}\")\n",
        "            print(f\"   Failed models: {row['failed_models']}\")\n",
        "            print(f\"   Content preview: {row['content'][:100]}...\")\n",
        "    \n",
        "    return deduplicated\n",
        "\n",
        "# Deduplicate the processed error cases\n",
        "if len(processed_errors) > 0:\n",
        "    deduplicated_errors = deduplicate_error_cases(processed_errors)\n",
        "    \n",
        "    if len(deduplicated_errors) > 0:\n",
        "        print(f\"\\\\n📋 Deduplicated dataset shape: {deduplicated_errors.shape}\")\n",
        "        print(f\"📋 Columns: {deduplicated_errors.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"❌ No data left after deduplication\")\n",
        "else:\n",
        "    print(\"❌ No processed errors to deduplicate\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Create Final Unified Dataset with Required Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Creating final unified dataset with required columns...\n",
            "✅ Final dataset created successfully!\n",
            "📏 Shape: (2540, 4)\n",
            "📋 Columns: ['id', 'content', 'label', 'source']\n",
            "\\n📊 Dataset Statistics:\n",
            "  LEGITIMATE (label=0): 1056 records (41.6%)\n",
            "  SCAM (label=1): 1484 records (58.4%)\n",
            "\\n📊 Source Distribution:\n",
            "  email: 1701 records (67.0%)\n",
            "  sms: 839 records (33.0%)\n",
            "\\n📝 Content Length Statistics:\n",
            "  Average length: 2060 characters\n",
            "  Median length: 356 characters\n",
            "  Min length: 3 characters\n",
            "  Max length: 112031 characters\n",
            "✅ All records have non-empty content\n",
            "\\n🎉 SUCCESS: Unified error dataset created!\n",
            "📋 Final dataset preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>&amp;lt;#&amp;gt;  w jetton ave if you forgot</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>&amp;lt;#&amp;gt; %of pple marry with their lovers... becz they hav gud undrstndng dat avoids problems. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>'Wnevr i wana fal in luv vth my books, My bed fals in luv vth me..!'' . Yen madodu, nav pretsorg...</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>(You didn't hear it from me)</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>, ow u dey.i paid 60,400thousad.i told  u would call .</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>08714712388 between 10am-7pm Cost 10p</td>\n",
              "      <td>1</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \\suppliers\\\"</td>\n",
              "      <td>1</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1's finish meeting call me.</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1's reach home call me.</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>10 min later k...</td>\n",
              "      <td>0</td>\n",
              "      <td>sms</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  \\\n",
              "0   1   \n",
              "1   2   \n",
              "2   3   \n",
              "3   4   \n",
              "4   5   \n",
              "5   6   \n",
              "6   7   \n",
              "7   8   \n",
              "8   9   \n",
              "9  10   \n",
              "\n",
              "                                                                                               content  \\\n",
              "0                                                                &lt;#&gt;  w jetton ave if you forgot   \n",
              "1  &lt;#&gt; %of pple marry with their lovers... becz they hav gud undrstndng dat avoids problems. ...   \n",
              "2  'Wnevr i wana fal in luv vth my books, My bed fals in luv vth me..!'' . Yen madodu, nav pretsorg...   \n",
              "3                                                                         (You didn't hear it from me)   \n",
              "4                                               , ow u dey.i paid 60,400thousad.i told  u would call .   \n",
              "5                                                                08714712388 between 10am-7pm Cost 10p   \n",
              "6          0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \\suppliers\\\"   \n",
              "7                                                                          1's finish meeting call me.   \n",
              "8                                                                              1's reach home call me.   \n",
              "9                                                                                    10 min later k...   \n",
              "\n",
              "   label source  \n",
              "0      0    sms  \n",
              "1      0    sms  \n",
              "2      0    sms  \n",
              "3      0    sms  \n",
              "4      0    sms  \n",
              "5      1    sms  \n",
              "6      1    sms  \n",
              "7      0    sms  \n",
              "8      0    sms  \n",
              "9      0    sms  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def create_final_unified_dataset(deduplicated_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create the final unified dataset with required columns: id, content, label, source\n",
        "    \"\"\"\n",
        "    if len(deduplicated_df) == 0:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"🎯 Creating final unified dataset with required columns...\")\n",
        "    \n",
        "    # Create the final dataset with required columns\n",
        "    final_dataset = pd.DataFrame()\n",
        "    \n",
        "    # Generate unique IDs for each data point\n",
        "    final_dataset['id'] = range(1, len(deduplicated_df) + 1)\n",
        "    \n",
        "    # Add content (already processed and deduplicated)\n",
        "    final_dataset['content'] = deduplicated_df['content'].values\n",
        "    \n",
        "    # Add label (0 = legitimate, 1 = scam)\n",
        "    final_dataset['label'] = deduplicated_df['label'].values\n",
        "    \n",
        "    # Add source (email, sms, or other)\n",
        "    final_dataset['source'] = deduplicated_df['source'].values\n",
        "    \n",
        "    # Validate the final dataset\n",
        "    print(f\"✅ Final dataset created successfully!\")\n",
        "    print(f\"📏 Shape: {final_dataset.shape}\")\n",
        "    print(f\"📋 Columns: {final_dataset.columns.tolist()}\")\n",
        "    \n",
        "    # Dataset statistics\n",
        "    print(f\"\\\\n📊 Dataset Statistics:\")\n",
        "    label_counts = final_dataset['label'].value_counts().sort_index()\n",
        "    total_records = len(final_dataset)\n",
        "    \n",
        "    for label, count in label_counts.items():\n",
        "        label_name = \"LEGITIMATE\" if label == 0 else \"SCAM\"\n",
        "        percentage = (count / total_records) * 100\n",
        "        print(f\"  {label_name} (label={label}): {count} records ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Source distribution statistics\n",
        "    print(f\"\\\\n📊 Source Distribution:\")\n",
        "    source_counts = final_dataset['source'].value_counts()\n",
        "    for source, count in source_counts.items():\n",
        "        percentage = (count / total_records) * 100\n",
        "        print(f\"  {source}: {count} records ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Content length statistics\n",
        "    content_lengths = final_dataset['content'].str.len()\n",
        "    print(f\"\\\\n📝 Content Length Statistics:\")\n",
        "    print(f\"  Average length: {content_lengths.mean():.0f} characters\")\n",
        "    print(f\"  Median length: {content_lengths.median():.0f} characters\")\n",
        "    print(f\"  Min length: {content_lengths.min()} characters\")\n",
        "    print(f\"  Max length: {content_lengths.max()} characters\")\n",
        "    \n",
        "    # Check for any empty content\n",
        "    empty_content = final_dataset[final_dataset['content'].str.strip() == '']\n",
        "    if len(empty_content) > 0:\n",
        "        print(f\"⚠️ Warning: Found {len(empty_content)} records with empty content\")\n",
        "    else:\n",
        "        print(f\"✅ All records have non-empty content\")\n",
        "    \n",
        "    return final_dataset\n",
        "\n",
        "# Create the final unified dataset\n",
        "if len(deduplicated_errors) > 0:\n",
        "    unified_error_dataset = create_final_unified_dataset(deduplicated_errors)\n",
        "    \n",
        "    if len(unified_error_dataset) > 0:\n",
        "        print(f\"\\\\n🎉 SUCCESS: Unified error dataset created!\")\n",
        "        print(f\"📋 Final dataset preview:\")\n",
        "        display(unified_error_dataset.head(10))\n",
        "    else:\n",
        "        print(\"❌ Failed to create final dataset\")\n",
        "else:\n",
        "    print(\"❌ No deduplicated data available for final dataset creation\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Additional Analysis and Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Creating additional metadata for analysis...\n",
            "✅ Extended dataset created with metadata\n",
            "📏 Extended dataset shape: (2540, 9)\n",
            "📋 Extended columns: ['id', 'content', 'label', 'source', 'source_datasets', 'failed_models', 'error_count', 'error_types', 'llm_reasoning']\n",
            "\\n🔍 ERROR PATTERN ANALYSIS:\n",
            "--------------------------------------------------\n",
            "\\n🚨 Data points that failed most frequently:\n",
            "\\n1. ID 1080 - Failed 39 times\n",
            "   Label: SCAM\n",
            "   Models that failed: gpt-4.1, Qwen/Qwen3-30B-A3B, Qwen/Qwen2.5-1.5B-Instruct, qwen/deepseek-r1-0528-qwen3-8b, unsloth/qwen3-30b-a3b, qwen3-235b-a22b-128k, unsloth/gemma-3-4b-it, qwen/qwen3-8b, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-4b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemini-2.5-flash-preview-05-20, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat, gemma3-1b-it\n",
            "   Content preview: Subject: CNN.com Daily Top 10\n",
            "\n",
            "Body: >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+= >...\n",
            "\\n2. ID 1665 - Failed 20 times\n",
            "   Label: SCAM\n",
            "   Models that failed: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen/deepseek-r1-0528-qwen3-8b, unsloth/qwen3-30b-a3b, qwen3-235b-a22b-128k, qwen/qwen3-8b, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-4b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Newsletter Coach'Invest - Septembre 2002\n",
            "\n",
            "Body: NEWSLETTER COACHINVEST  SEPTEMBRE 2002\n",
            "\n",
            "Coa...\n",
            "\\n3. ID 1254 - Failed 16 times\n",
            "   Label: SCAM\n",
            "   Models that failed: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: failure notice\n",
            "\n",
            "Body: hi . this is the qmail - send program at bouncehost .\n",
            "i ' m afraid i ...\n",
            "\\n4. ID 1512 - Failed 16 times\n",
            "   Label: SCAM\n",
            "   Models that failed: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Introduction Regular Expressions are a widely-used method of specifying patterns of text to...\n",
            "\\n5. ID 1867 - Failed 16 times\n",
            "   Label: SCAM\n",
            "   Models that failed: gpt-4.1, Qwen/Qwen3-30B-A3B, qwen3-235b-a22b-128k, qwen/qwen3-235b-a22b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat, unsloth/qwen3-30b-a3b, llama-4-maverick-17b-128e-instruct, llama-4-scout-17b-16e-instruct, unsloth/qwen3-32b, gemma-3-27b-it, qwen3-0.6b, deepseek-r1-0528-distill-qwen3-32b-preview0-qat\n",
            "   Content preview: Subject: Re: Re:\n",
            "\n",
            "Body: The cop looked like a big doll that has been badly treated by a gang of nast...\n",
            "\\n📊 Error distribution by source dataset:\n",
            "  phishing_sms_dataset: 839 unique error data points\n",
            "  unified_phishing_email_dataset: 1701 unique error data points\n",
            "\\n📊 Error type distribution:\n",
            "  False Positive: 1070 data points\n",
            "  False Negative: 1471 data points\n",
            "\\n📝 Content length analysis by label:\n",
            "  LEGITIMATE: Average 2588 characters\n",
            "  SCAM: Average 1684 characters\n",
            "\\n📋 Sample of extended dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "      <th>error_count</th>\n",
              "      <th>failed_models</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>&amp;lt;#&amp;gt;  w jetton ave if you forgot</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Deepseek-r1-0528-distill-qwen3-32b-preview0-qat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>&amp;lt;#&amp;gt; %of pple marry with their lovers... becz they hav gud undrstndng dat avoids problems. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>unsloth/qwen3-30b-a3b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>'Wnevr i wana fal in luv vth my books, My bed fals in luv vth me..!'' . Yen madodu, nav pretsorg...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>unsloth/qwen3-30b-a3b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>(You didn't hear it from me)</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>unsloth/qwen3-30b-a3b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>, ow u dey.i paid 60,400thousad.i told  u would call .</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>unsloth/qwen3-30b-a3b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  \\\n",
              "0   1   \n",
              "1   2   \n",
              "2   3   \n",
              "3   4   \n",
              "4   5   \n",
              "\n",
              "                                                                                               content  \\\n",
              "0                                                                &lt;#&gt;  w jetton ave if you forgot   \n",
              "1  &lt;#&gt; %of pple marry with their lovers... becz they hav gud undrstndng dat avoids problems. ...   \n",
              "2  'Wnevr i wana fal in luv vth my books, My bed fals in luv vth me..!'' . Yen madodu, nav pretsorg...   \n",
              "3                                                                         (You didn't hear it from me)   \n",
              "4                                               , ow u dey.i paid 60,400thousad.i told  u would call .   \n",
              "\n",
              "   label  error_count  \\\n",
              "0      0            1   \n",
              "1      0            2   \n",
              "2      0            1   \n",
              "3      0            1   \n",
              "4      0            2   \n",
              "\n",
              "                                                            failed_models  \n",
              "0                         Deepseek-r1-0528-distill-qwen3-32b-preview0-qat  \n",
              "1  unsloth/qwen3-30b-a3b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat  \n",
              "2                                                   unsloth/qwen3-30b-a3b  \n",
              "3                                                   unsloth/qwen3-30b-a3b  \n",
              "4  unsloth/qwen3-30b-a3b, Deepseek-r1-0528-distill-qwen3-32b-preview0-qat  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create additional metadata dataset for analysis\n",
        "if len(deduplicated_errors) > 0 and len(unified_error_dataset) > 0:\n",
        "    print(\"📋 Creating additional metadata for analysis...\")\n",
        "    \n",
        "    # Create extended dataset with metadata (for optional analysis)\n",
        "    extended_dataset = unified_error_dataset.copy()\n",
        "    \n",
        "    # Add metadata from the deduplicated errors\n",
        "    extended_dataset['source_datasets'] = deduplicated_errors['source_datasets'].values\n",
        "    extended_dataset['failed_models'] = deduplicated_errors['failed_models'].values\n",
        "    extended_dataset['error_count'] = deduplicated_errors['error_count'].values\n",
        "    extended_dataset['error_types'] = deduplicated_errors['error_types'].values\n",
        "    extended_dataset['llm_reasoning'] = deduplicated_errors['llm_reason'].values\n",
        "    \n",
        "    print(f\"✅ Extended dataset created with metadata\")\n",
        "    print(f\"📏 Extended dataset shape: {extended_dataset.shape}\")\n",
        "    print(f\"📋 Extended columns: {extended_dataset.columns.tolist()}\")\n",
        "    \n",
        "    # Show analysis of error patterns\n",
        "    print(f\"\\\\n🔍 ERROR PATTERN ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Most frequently failed data points\n",
        "    frequent_errors = extended_dataset.nlargest(5, 'error_count')\n",
        "    print(f\"\\\\n🚨 Data points that failed most frequently:\")\n",
        "    for i, (_, row) in enumerate(frequent_errors.iterrows(), 1):\n",
        "        print(f\"\\\\n{i}. ID {row['id']} - Failed {row['error_count']} times\")\n",
        "        print(f\"   Label: {'SCAM' if row['label'] == 1 else 'LEGITIMATE'}\")\n",
        "        print(f\"   Models that failed: {row['failed_models']}\")\n",
        "        print(f\"   Content preview: {row['content'][:100]}...\")\n",
        "    \n",
        "    # Error distribution by original dataset\n",
        "    print(f\"\\\\n📊 Error distribution by source dataset:\")\n",
        "    for dataset in deduplicated_errors['source_datasets'].unique():\n",
        "        # Count how many errors came from each dataset\n",
        "        dataset_errors = extended_dataset[extended_dataset['source_datasets'].str.contains(dataset)]\n",
        "        print(f\"  {dataset}: {len(dataset_errors)} unique error data points\")\n",
        "    \n",
        "    # Error type distribution\n",
        "    print(f\"\\\\n📊 Error type distribution:\")\n",
        "    error_type_counts = {}\n",
        "    for error_types in extended_dataset['error_types'].values:\n",
        "        for error_type in error_types.split(', '):\n",
        "            error_type_counts[error_type] = error_type_counts.get(error_type, 0) + 1\n",
        "    \n",
        "    for error_type, count in error_type_counts.items():\n",
        "        print(f\"  {error_type}: {count} data points\")\n",
        "    \n",
        "    # Content length analysis by label\n",
        "    print(f\"\\\\n📝 Content length analysis by label:\")\n",
        "    for label in [0, 1]:\n",
        "        label_data = unified_error_dataset[unified_error_dataset['label'] == label]\n",
        "        label_name = \"LEGITIMATE\" if label == 0 else \"SCAM\"\n",
        "        if len(label_data) > 0:\n",
        "            avg_length = label_data['content'].str.len().mean()\n",
        "            print(f\"  {label_name}: Average {avg_length:.0f} characters\")\n",
        "    \n",
        "    print(f\"\\\\n📋 Sample of extended dataset:\")\n",
        "    display(extended_dataset[['id', 'content', 'label', 'error_count', 'failed_models']].head())\n",
        "\n",
        "else:\n",
        "    print(\"❌ No data available for extended analysis\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Save the Unified Error Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Saving unified error dataset...\n",
            "✅ Main dataset saved: unified_error_dataset/unified_error_dataset.csv\n",
            "   Columns: ['id', 'content', 'label', 'source']\n",
            "   Shape: (2540, 4)\n",
            "✅ Extended dataset saved: unified_error_dataset/unified_error_dataset_with_metadata.csv\n",
            "   Columns: ['id', 'content', 'label', 'source', 'source_datasets', 'failed_models', 'error_count', 'error_types', 'llm_reasoning']\n",
            "   Shape: (2540, 9)\n",
            "✅ Dataset info saved: unified_error_dataset/dataset_info.json\n"
          ]
        }
      ],
      "source": [
        "# Save the datasets\n",
        "if len(unified_error_dataset) > 0:\n",
        "    print(\"💾 Saving unified error dataset...\")\n",
        "    \n",
        "    # Create output directory\n",
        "    output_dir = Path('unified_error_dataset')\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save the main dataset (id, content, label only)\n",
        "    main_file = output_dir / 'unified_error_dataset.csv'\n",
        "    unified_error_dataset.to_csv(main_file, index=False)\n",
        "    print(f\"✅ Main dataset saved: {main_file}\")\n",
        "    print(f\"   Columns: {unified_error_dataset.columns.tolist()}\")\n",
        "    print(f\"   Shape: {unified_error_dataset.shape}\")\n",
        "    \n",
        "    # Save the extended dataset with metadata (if available)\n",
        "    if 'extended_dataset' in locals() and len(extended_dataset) > 0:\n",
        "        extended_file = output_dir / 'unified_error_dataset_with_metadata.csv'\n",
        "        extended_dataset.to_csv(extended_file, index=False)\n",
        "        print(f\"✅ Extended dataset saved: {extended_file}\")\n",
        "        print(f\"   Columns: {extended_dataset.columns.tolist()}\")\n",
        "        print(f\"   Shape: {extended_dataset.shape}\")\n",
        "    \n",
        "    # Save dataset information\n",
        "    info_file = output_dir / 'dataset_info.json'\n",
        "    dataset_info = {\n",
        "        'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_records': len(unified_error_dataset),\n",
        "        'label_distribution': {\n",
        "            'legitimate': int(unified_error_dataset[unified_error_dataset['label'] == 0].shape[0]),\n",
        "            'scam': int(unified_error_dataset[unified_error_dataset['label'] == 1].shape[0])\n",
        "        },\n",
        "        'content_length_stats': {\n",
        "            'mean': float(unified_error_dataset['content'].str.len().mean()),\n",
        "            'median': float(unified_error_dataset['content'].str.len().median()),\n",
        "            'min': int(unified_error_dataset['content'].str.len().min()),\n",
        "            'max': int(unified_error_dataset['content'].str.len().max())\n",
        "        },\n",
        "        'source_info': {\n",
        "            'total_original_errors': len(all_errors) if len(all_errors) > 0 else 0,\n",
        "            'after_processing': len(processed_errors) if len(processed_errors) > 0 else 0,\n",
        "            'after_deduplication': len(deduplicated_errors) if len(deduplicated_errors) > 0 else 0,\n",
        "            'duplicates_removed': len(processed_errors) - len(deduplicated_errors) if len(processed_errors) > 0 and len(deduplicated_errors) > 0 else 0\n",
        "        },\n",
        "        'columns': {\n",
        "            'main_dataset': unified_error_dataset.columns.tolist(),\n",
        "            'extended_dataset': extended_dataset.columns.tolist() if 'extended_dataset' in locals() else []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(info_file, 'w') as f:\n",
        "        json.dump(dataset_info, f, indent=2)\n",
        "    print(f\"✅ Dataset info saved: {info_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
